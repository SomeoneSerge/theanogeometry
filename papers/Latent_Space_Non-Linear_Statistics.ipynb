{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This file is part of Theano Geometry\n",
    "#\n",
    "# Copyright (C) 2017, Stefan Sommer (sommer@di.ku.dk)\n",
    "# https://bitbucket.org/stefansommer/theanogemetry\n",
    "#\n",
    "# Theano Geometry is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# Theano Geometry is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with Theano Geometry. If not, see <http://www.gnu.org/licenses/>.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space Geometry\n",
    "arXiv:1805.07632 [cs.CV] https://arxiv.org/abs/1805.07632\n",
    "\n",
    "Line Kuhnel, Tom Fletcher, Sarang Joshi, and Stefan Sommer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Keras / Theano Geometry compatibility\n",
    "from keras import backend as K\n",
    "import theano\n",
    "print(theano.config.floatX)\n",
    "print(K.floatx())\n",
    "assert(theano.config.floatX == K.floatx())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S2 or MNIST example\n",
    "name = 'mnist'\n",
    "# name = 'S2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if name is 'S2':\n",
    "    ## setup neural network\n",
    "\n",
    "    # adapted from Keras variational_autoencoder.py example \n",
    "    # https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py\n",
    "    '''This script demonstrates how to build a variational autoencoder with Keras.\n",
    "     #Reference\n",
    "     - Auto-Encoding Variational Bayes\n",
    "       https://arxiv.org/abs/1312.6114\n",
    "    '''\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.stats import norm\n",
    "\n",
    "    from keras.layers import Input, Dense, Lambda\n",
    "    from keras.models import Model\n",
    "    from keras import backend as K\n",
    "    from keras import metrics\n",
    "    from keras import optimizers\n",
    "\n",
    "    batch_size = 100\n",
    "    original_dim = 3\n",
    "    latent_dim = 2\n",
    "    intermediate_dim = 128\n",
    "    epochs = 1000\n",
    "    epsilon_std = .1\n",
    "\n",
    "    x = Input(shape=(original_dim,))\n",
    "    encoder_h = Dense(intermediate_dim, activation='tanh')\n",
    "    h = encoder_h(x)\n",
    "    encoder_z_mean = Dense(latent_dim)\n",
    "    z_mean = encoder_z_mean(h)\n",
    "    z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                                  stddev=epsilon_std)\n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # we instantiate these layers separately so as to reuse them later\n",
    "    decoder_h = Dense(intermediate_dim, activation='tanh')\n",
    "    decoder_mean = Dense(original_dim, activation='tanh')\n",
    "    h_decoded = decoder_h(z)\n",
    "    x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "    # instantiate VAE model\n",
    "    vae = Model(x, x_decoded_mean)\n",
    "\n",
    "    # Compute VAE loss\n",
    "    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer=optimizers.RMSprop(lr=0.01,))\n",
    "    vae.summary()\n",
    "\n",
    "    # train the VAE on MNIST digits\n",
    "    x_train = np.load('S2_samples.npy')\n",
    "    x_test = x_train[3*x_train.shape[0]//4:-1,:]\n",
    "    x_train = x_train[0:3*x_train.shape[0]//4,:]\n",
    "\n",
    "    vae.fit(x_train,\n",
    "            shuffle=True,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(x_test, None),\n",
    "            verbose=0)\n",
    "\n",
    "    # build a model to project inputs on the latent space\n",
    "    encoder = Model(x, z_mean)\n",
    "\n",
    "    # display a 2D plot of the data in the latent space\n",
    "    x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1])\n",
    "    plt.show()\n",
    "\n",
    "    # build a digit generator that can sample from the learned distribution\n",
    "    decoder_input = Input(shape=(latent_dim,))\n",
    "    _h_decoded = decoder_h(decoder_input)\n",
    "    _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "    generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "    # decode test data\n",
    "    x_test_decoded = generator.predict(x_test_encoded, batch_size=batch_size)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    ax = plt.gca(projection='3d')\n",
    "    ax.scatter(x_test_decoded[:,0],x_test_decoded[:,1],x_test_decoded[:,2],color='red')\n",
    "    ax.scatter(x_test[:,0],x_test[:,1],x_test[:,2],color='blue')\n",
    "    ax.set_aspect(\"equal\")\n",
    "#     plt.savefig('S2_test.pdf')\n",
    "    plt.show()\n",
    "\n",
    "    # display the image of the latent space in the embedding space\n",
    "    n = 15\n",
    "    points = np.zeros((n,n,3))\n",
    "    # linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n",
    "    # to produce values of the latent variables z, since the prior of the latent space is Gaussian\n",
    "    grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "    grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = generator.predict(z_sample)\n",
    "            points[i,j] = x_decoded[0]\n",
    "    points = points.reshape((-1,3))\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    ax = plt.gca(projection='3d')\n",
    "    ax.scatter(points[:,0],points[:,1],points[:,2],color='red')\n",
    "    ax.scatter(x_test[:,0],x_test[:,1],x_test[:,2],color='blue')\n",
    "    ax.set_aspect(\"equal\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if name is 'mnist':\n",
    "    ## setup neural network\n",
    "\n",
    "    # adapted from Keras variational_autoencoder.py example \n",
    "    # https://github.com/keras-team/keras/blob/master/examples/variational_autoencoder.py\n",
    "    '''This script demonstrates how to build a variational autoencoder with Keras.\n",
    "     #Reference\n",
    "     - Auto-Encoding Variational Bayes\n",
    "       https://arxiv.org/abs/1312.6114\n",
    "    '''\n",
    "\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.stats import norm\n",
    "\n",
    "    from keras.layers import Input, Dense, Lambda\n",
    "    from keras.models import Model\n",
    "    from keras import backend as K\n",
    "    from keras import metrics\n",
    "    from keras.datasets import mnist\n",
    "\n",
    "    batch_size = 100\n",
    "    original_dim = 784\n",
    "    latent_dim = 2\n",
    "    intermediate_dim = 256\n",
    "    epochs = 50\n",
    "    epsilon_std = 1.0\n",
    "\n",
    "    x = Input(shape=(original_dim,))\n",
    "    encoder_h = Dense(intermediate_dim, activation='tanh')\n",
    "    h = encoder_h(x)\n",
    "    encoder_z_mean = Dense(latent_dim)\n",
    "    z_mean = encoder_z_mean(h)\n",
    "    z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                                  stddev=epsilon_std)\n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # we instantiate these layers separately so as to reuse them later\n",
    "    decoder_h = Dense(intermediate_dim, activation='tanh')\n",
    "    decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "    h_decoded = decoder_h(z)\n",
    "    x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "    # instantiate VAE model\n",
    "    vae = Model(x, x_decoded_mean)\n",
    "\n",
    "    # Compute VAE loss\n",
    "    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='rmsprop')\n",
    "    vae.summary()\n",
    "\n",
    "    # train the VAE on MNIST digits\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "    x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "    vae.fit(x_train,\n",
    "            shuffle=True,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(x_test, None))\n",
    "\n",
    "    # build a model to project inputs on the latent space\n",
    "    encoder = Model(x, z_mean)\n",
    "\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # build a digit generator that can sample from the learned distribution\n",
    "    decoder_input = Input(shape=(latent_dim,))\n",
    "    _h_decoded = decoder_h(decoder_input)\n",
    "    _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "    generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "    # display a 2D manifold of the digits\n",
    "    n = 15  # figure with 15x15 digits\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n",
    "    # to produce values of the latent variables z, since the prior of the latent space is Gaussian\n",
    "    grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "    grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = generator.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build a generator for Theano Geometry\n",
    "def F(x):\n",
    "    decoder_input = Input(shape=(latent_dim,),tensor=x.reshape((1,-1)))\n",
    "    _h_decoded = decoder_h(decoder_input)\n",
    "    _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "    generator = Model(decoder_input, _x_decoded_mean)\n",
    "    return generator.layers[-1].get_output_at(-1).flatten()\n",
    "\n",
    "# build inverse mapping\n",
    "def invF(y):\n",
    "    x = Input(shape=(original_dim,),tensor=y.reshape((1,-1)))\n",
    "    h = encoder_h(x)\n",
    "    z_mean = encoder_z_mean(h)\n",
    "    encoder = Model(x, z_mean)\n",
    "    return encoder.layers[-1].get_output_at(-1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from src.manifolds.latent import *\n",
    "M = Latent(F,latent_dim,original_dim,invF=invF)\n",
    "print(M)\n",
    "\n",
    "from src.plotting import *\n",
    "if name is 'S2':\n",
    "    M.newfig()\n",
    "    M.plot(alpha=.5)\n",
    "#     plt.savefig(name+'_learned_manifold.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# learn metric structure\n",
    "\n",
    "# Riemannian structure\n",
    "from src.Riemannian import metric\n",
    "metric.initialize(M)\n",
    "# save metric\n",
    "if not hasattr(M,'g_org'):\n",
    "    M.g_org = M.g\n",
    "    M.g_orgf = M.gf    \n",
    "    M.gsharp_org = M.gsharp\n",
    "    M.gsharp_orgf = M.gsharpf\n",
    "\n",
    "# triangular matrix functions\n",
    "r = T.arange(M.dim)\n",
    "tmp_mat = r[np.newaxis, :] + ((M.dim * (M.dim - 3)) // 2-(r * (r - 1)) // 2)[::-1,np.newaxis]\n",
    "triu_index_matrix = T.triu(tmp_mat+1)-T.diag(T.diagonal(tmp_mat+1))\n",
    "\n",
    "def VtoLA(hatxi): # from \\RR^G_dim to LA\n",
    "    if hatxi.type == T.vector().type:\n",
    "        m = T.concatenate((T.zeros(1),hatxi[M.dim:]))[triu_index_matrix]\n",
    "        return m+m.T+T.diag(hatxi[:M.dim])\n",
    "    else: # matrix\n",
    "        N = hatxi.shape[0]\n",
    "        m = T.concatenate((T.zeros((1,hatxi.shape[0])),hatxi[:,M.dim:].T))[triu_index_matrix,:]\n",
    "        return (m+m.dimshuffle((1,0,2))).dimshuffle((2,0,1))+T.tile(hatxi[:,:M.dim].reshape((N,M.dim,1)),(1,1,int(M.dim.eval())))*T.tile(T.eye(M.dim).reshape((1,M.dim,M.dim)),(N,1,1))    \n",
    "LAtoV = lambda m: T.concatenate((T.diag(m),m[T.triu(T.ones((int(M.dim.eval()),int(M.dim.eval())))-T.diag(T.ones(M.dim))).nonzero()])) # from LA to \\RR^G_dim\n",
    "\n",
    "hatxi = T.vector()\n",
    "VtoLAf = theano.function([hatxi],VtoLA(hatxi))\n",
    "# # VtoLAf(np.array([[1.,2.,3.],[-1.,-2.,-3.]]))\n",
    "mm = T.matrix()\n",
    "LAtoVf = theano.function([mm],LAtoV(mm))\n",
    "# print(VtoLAf(np.array([1.,2.,3.])))\n",
    "# LAtoVf(VtoLAf(np.array([1.,2.,3.])))\n",
    "\n",
    "# samples, n sample points per axis\n",
    "n = 301\n",
    "samples = np.zeros((n,n,M.dim.eval()))\n",
    "g_samples = np.zeros((n,n,2,M.dim.eval()*(M.dim.eval()+1)//2))\n",
    "\n",
    "# grid\n",
    "from scipy.stats import norm\n",
    "xmin = np.min(x_test_encoded[:,0])\n",
    "xmax = np.max(x_test_encoded[:,0])\n",
    "ymin = np.min(x_test_encoded[:,1])\n",
    "ymax = np.max(x_test_encoded[:,1])\n",
    "if name is 'S2':\n",
    "    grid_x = norm.ppf(np.linspace(0.005, 0.995, n))\n",
    "    grid_y = norm.ppf(np.linspace(0.005, 0.995, n))    \n",
    "if name is 'mnist':\n",
    "    grid_x = 1.1*np.linspace(xmin,xmax,n)\n",
    "    grid_y = 1.1*np.linspace(ymin,ymax,n)\n",
    "print(n)\n",
    "xmin = np.min(grid_x)\n",
    "xmax = np.max(grid_x)\n",
    "ymin = np.min(grid_y)\n",
    "ymax = np.max(grid_y)\n",
    "print(xmin,xmax,ymin,ymax)\n",
    "\n",
    "for i, xi in enumerate(grid_x):\n",
    "    for j, yi in enumerate(grid_y):\n",
    "        samples[i,j] = np.array([[xi, yi]])\n",
    "        g = M.g_orgf(samples[i,j].astype(theano.config.floatX))\n",
    "        g_samples[i,j][0] = LAtoVf(M.g_orgf(samples[i,j].astype(theano.config.floatX)))\n",
    "        g_samples[i,j][1] = LAtoVf(M.gsharp_orgf(samples[i,j].astype(theano.config.floatX)))\n",
    "samples = samples.reshape((n**2, np.prod(samples.shape[2:])))\n",
    "g_samples = g_samples.reshape((n**2, np.prod(g_samples.shape[2:])))\n",
    "\n",
    "g_input = Input(shape=(latent_dim,))\n",
    "if name is 'S2':\n",
    "    layer_h_g = Dense(512, activation='tanh')\n",
    "if name is 'mnist':\n",
    "    layer_h_g = Dense(2048, activation='tanh')\n",
    "h_g = layer_h_g(g_input)\n",
    "layer_h_g2 = Dense(8, activation='tanh')\n",
    "h_g2 = layer_h_g2(h_g)\n",
    "layer_g_predicted = Dense(2*M.dim.eval()*(M.dim.eval()+1)//2, activation='linear')\n",
    "g_predicted = layer_g_predicted(h_g2)\n",
    "g_predictor = Model(g_input,g_predicted)\n",
    "\n",
    "# g and gsharp differencce\n",
    "def loss_diff(yTrue,yPred):\n",
    "    yTrue = yTrue.reshape((-1,2,M.dim*(M.dim+1)//2))\n",
    "    yPred = yPred.reshape((-1,2,M.dim*(M.dim+1)//2))\n",
    "    N = yTrue.shape[0]\n",
    "    gTrue = VtoLA(yTrue[:,0])\n",
    "    gsharpTrue = VtoLA(yTrue[:,1])    \n",
    "    gPred = VtoLA(yPred[:,0])\n",
    "    gsharpPred = VtoLA(yPred[:,1])\n",
    "    return ( K.mean((T.sum(K.square(gPred-gTrue),(1,2))/T.sum(abs(gTrue**2),(1,2))).flatten())\n",
    "            +K.mean((T.sum(K.square(gsharpPred-gsharpTrue),(1,2))/T.sum(abs(gsharpTrue**2),(1,2))).flatten()))\n",
    "# g and gsharp differencce and measure of g,gsharp being inverses\n",
    "def loss_inverse(yTrue,yPred):\n",
    "    yTrue = yTrue.reshape((-1,2,M.dim*(M.dim+1)//2))\n",
    "    yPred = yPred.reshape((-1,2,M.dim*(M.dim+1)//2))\n",
    "    N = yTrue.shape[0]\n",
    "    gPred = VtoLA(yPred[:,0])\n",
    "    gsharpPred = VtoLA(yPred[:,1])\n",
    "    return K.mean(K.square(K.batch_dot(gsharpPred,gPred,(2,1))-K.tile(K.eye(M.dim.eval()).reshape((1,M.dim,M.dim)),(N,1,1))).flatten())\n",
    "loss_init = loss_diff\n",
    "loss = lambda yTrue,yPred: loss_diff(yTrue,yPred)+loss_inverse(yTrue,yPred)\n",
    "\n",
    "g_predictor_init = Model(g_input,g_predicted)\n",
    "g_predictor_init.compile(loss=loss_init,optimizer='rmsprop',metrics=[loss_inverse]) # run some iterations of this to initialize weights before using custom loss\n",
    "g_predictor.compile(loss=loss,optimizer='rmsprop',metrics=[loss_diff,loss_inverse])\n",
    "g_predictor.summary()\n",
    "\n",
    "# train\n",
    "g_predictor_init.fit(samples,g_samples,\n",
    "        shuffle=True,\n",
    "        epochs=2000,\n",
    "        batch_size=n**2//100,\n",
    "        verbose=0\n",
    "#         validation_data=(x_test, None)\n",
    "               )\n",
    "print(\"training result init: %s\" % g_predictor_init.evaluate(samples,g_samples))\n",
    "x = np.array([0.,0.]).astype(theano.config.floatX)\n",
    "print(M.g_orgf(x))\n",
    "print(M.gsharp_orgf(x))\n",
    "print(g_predictor.predict(np.array([0.,0.]).astype(theano.config.floatX).reshape((1,-1))))\n",
    "g_predictor.fit(samples,g_samples,\n",
    "        shuffle=True,\n",
    "        epochs=2000,\n",
    "        batch_size=n**2//100,\n",
    "        verbose=0\n",
    "#         validation_data=(x_test, None)\n",
    "               )\n",
    "print(\"training result: %s\" % g_predictor.evaluate(samples,g_samples))\n",
    "print(M.g_orgf(x))\n",
    "print(np.linalg.norm(M.g_orgf(x),-2))\n",
    "print(M.gsharp_orgf(x))\n",
    "print(np.linalg.norm(M.gsharp_orgf(x),-2))\n",
    "print(g_predictor.predict(np.array([0.,0.]).astype(theano.config.floatX).reshape((1,-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new metric and cometric and add to M\n",
    "\n",
    "inside = lambda x: T.and_(T.and_(T.ge(x[0],xmin),T.le(x[0],xmax)),T.and_(T.ge(x[1],ymin),T.le(x[1],ymax)))\n",
    "\n",
    "def g(x):\n",
    "    g_input = Input(shape=(latent_dim,),tensor=x.reshape((1,-1)))\n",
    "    h_g = layer_h_g(g_input)\n",
    "    h_g2 = layer_h_g2(h_g)\n",
    "    g_predicted = layer_g_predicted(h_g2)\n",
    "    g_predictor = Model(g_input,g_predicted)\n",
    "\n",
    "#     return theano.ifelse.ifelse(1,VtoLA(g_predictor.layers[-1].get_output_at(-1).reshape((2,-1))[0]),M.g_org(x))\n",
    "    return VtoLA(g_predictor.layers[-1].get_output_at(-1).reshape((2,-1))[0])\n",
    "def gsharp(x):\n",
    "    g_input = Input(shape=(latent_dim,),tensor=x.reshape((1,-1)))\n",
    "    h_g = layer_h_g(g_input)\n",
    "    h_g2 = layer_h_g2(h_g)\n",
    "    g_predicted = layer_g_predicted(h_g2)    \n",
    "    g_predictor = Model(g_input,g_predicted)\n",
    "\n",
    "#     return theano.ifelse.ifelse(inside(x),VtoLA(g_predictor.layers[-1].get_output_at(-1).reshape((2,-1))[1]),M.gsharp_org(x))\n",
    "    return VtoLA(g_predictor.layers[-1].get_output_at(-1).reshape((2,-1))[1])\n",
    "\n",
    "zz = T.vector()\n",
    "gf = theano.function([zz],g(zz))\n",
    "gsharpf = theano.function([zz],gsharp(zz))\n",
    "M.g = g\n",
    "M.gf = gf\n",
    "M.gsharp = gsharp\n",
    "M.gsharpf = gsharpf\n",
    "\n",
    "# re-initialize Riemannian structure\n",
    "metric.initialize(M)\n",
    "\n",
    "# some values\n",
    "x = np.array([0.,0.]).astype(theano.config.floatX)\n",
    "print(M.g_orgf(x))\n",
    "print(M.gf(x))\n",
    "print(M.g_orgf(x)-M.gf(x))\n",
    "print(np.linalg.eigvals(M.g_orgf(x)))\n",
    "print(np.linalg.eigvals(M.gf(x)))\n",
    "print()\n",
    "print(M.gsharp_orgf(x))\n",
    "print(M.gsharpf(x))\n",
    "print(M.gsharp_orgf(x)-M.gsharpf(x))\n",
    "print(np.linalg.eigvals(M.gsharp_orgf(x)))\n",
    "print(np.linalg.eigvals(M.gsharpf(x)))\n",
    "print()\n",
    "print(np.dot(M.gsharpf(x),M.gf(x)))\n",
    "print(np.dot(M.gf(x),M.gsharpf(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riemannian Geodesics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# element, tangent vector and covector\n",
    "x = np.array([0.,0.]).astype(theano.config.floatX)\n",
    "v = np.array([1.,1.]).astype(theano.config.floatX)\n",
    "p = M.flatf(x,v)\n",
    "\n",
    "print(\"x = \", x)\n",
    "print(\"v = \", v)\n",
    "print(\"p = \", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2nd order geodesic equation\n",
    "from src.Riemannian import geodesic\n",
    "geodesic.initialize(M)\n",
    "\n",
    "# compute geodesics\n",
    "xs = M.Exptf(x,v)\n",
    "\n",
    "# plot\n",
    "M.newfig()\n",
    "M.plot()\n",
    "M.plotx(xs,v,linewidth = 1.5, s=50)\n",
    "# plt.savefig(name+'_geodesic.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geodesics from Hamiltonian equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hamiltonian dynamics\n",
    "q = x\n",
    "print(M.Hf(q,p))\n",
    "\n",
    "from src.dynamics import Hamiltonian\n",
    "Hamiltonian.initialize(M)\n",
    "\n",
    "# Exponential map from Hamiltonian equations\n",
    "qs = M.Exp_Hamiltoniantf(q,p).T\n",
    "\n",
    "# plot\n",
    "M.newfig()\n",
    "M.plot()\n",
    "M.plotx(qs,v)\n",
    "plt.show()\n",
    "\n",
    "# dynamics returning both position and momentum\n",
    "(ts,qps) = M.Hamiltonian_dynamicsf(q,p)\n",
    "ps = qps[:,1,:]\n",
    "print(\"Energy: \",np.array([M.Hf(q,p) for (q,p) in zip(qs,ps)]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# approximate embedding space geodesic equation, see arXiv:1711.08014\n",
    "from src.Riemannian import geodesic_embedded\n",
    "geodesic_embedded.initialize(M)\n",
    "\n",
    "# compute geodesics\n",
    "xs = M.Exp_embeddedtf(x,v)\n",
    "\n",
    "# plot\n",
    "M.newfig()\n",
    "M.plot()\n",
    "M.plotx(xs,v,linewidth = 1.5, s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time results of different geodesic schemes\n",
    "\n",
    "# %time xs = M.Exp_embeddedtf(x,v)\n",
    "%time xs = M.Exp_Hamiltoniantf(x,v)\n",
    "%time xs = M.Exptf(x,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.Riemannian import curvature\n",
    "curvature.initialize(M)\n",
    "# Curvature tensor, Ricci and scalar curvature:\n",
    "print(\"curvature = \", M.Rf(x))\n",
    "print(\"Ricci curvature = \", M.Ricci_curvf(x))\n",
    "print(\"Scalar curvature = \", M.S_curvf(x))\n",
    "\n",
    "# Orthonormal basis under g:\n",
    "nu = M.gramSchmidt(x,np.eye(2).astype(theano.config.floatX)) # or nu = np.linalg.cholesky(M.gsharpf(x))\n",
    "\n",
    "# Sectional Curvature\n",
    "print(\"sectional curvature = \",M.sec_curvf(x,nu[:,0],nu[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot min of Ricci curvature tensor\n",
    "\n",
    "# grids\n",
    "pts = 200 # high value implies nicer plot but extended computation time\n",
    "if name is 'S2':\n",
    "    border = 1.\n",
    "if name is 'mnist':\n",
    "    border = 2.\n",
    "minx = -border\n",
    "maxx = +border\n",
    "miny = -border\n",
    "maxy = +border\n",
    "X, Y = np.meshgrid(np.linspace(minx,maxx,pts),np.linspace(miny,maxy,pts))\n",
    "xy = np.vstack([X.ravel(), Y.ravel()]).T        \n",
    "\n",
    "if name is 'S2':\n",
    "    vmin = -2.\n",
    "    vmax = 2.   \n",
    "if name is 'mnist':\n",
    "    vmin = -.6\n",
    "    vmax = .6\n",
    "    \n",
    "norm = mpl.colors.Normalize(vmin=vmin,vmax=vmax)\n",
    "\n",
    "# plot\n",
    "newfig()\n",
    "cmap = cm.jet\n",
    "alpha = 1\n",
    "ax = plt.gca()\n",
    "fs = np.array([M.S_curvf(x.astype(theano.config.floatX)) for x in xy])\n",
    "# norm = mpl.colors.Normalize(vmin=np.min(fs),vmax=np.max(fs))\n",
    "colors = cmap(norm(fs)).reshape(X.shape+(4,))\n",
    "surf = ax.plot_surface(X, Y, fs.reshape(X.shape), rstride=1, cstride=1, cmap=cmap, facecolors = colors,  linewidth=0., antialiased=True, alpha=alpha, edgecolor=(0,0,0,0), shade=False)\n",
    "m = cm.ScalarMappable(cmap=surf.cmap,norm=norm)\n",
    "m.set_array(colors)\n",
    "# plt.colorbar(m, shrink=0.7)\n",
    "ax.set_xlim3d(minx,maxx), ax.set_ylim3d(miny,maxy), ax.set_zlim3d(vmin,vmax)#np.min(fs)-1,np.max(fs)+1)\n",
    "# plt.savefig(name+'_scalar_curvature.pdf')\n",
    "plt.show()\n",
    "\n",
    "# plot\n",
    "newfig()\n",
    "cmap = cm.jet\n",
    "alpha = 1\n",
    "ax = plt.gca()\n",
    "fs = np.array([np.min(np.real(np.linalg.eigvals(np.dot(M.gsharpf(x.astype(theano.config.floatX)),M.Ricci_curvf(x.astype(theano.config.floatX)))))) for x in xy])\n",
    "# norm = mpl.colors.Normalize(vmin=np.min(fs),vmax=np.max(fs))\n",
    "colors = cmap(norm(fs)).reshape(X.shape+(4,))\n",
    "surf = ax.plot_surface(X, Y, fs.reshape(X.shape), rstride=1, cstride=1, cmap=cmap, facecolors = colors,  linewidth=0., antialiased=True, alpha=alpha, edgecolor=(0,0,0,0), shade=False)\n",
    "m = cm.ScalarMappable(cmap=surf.cmap,norm=norm)\n",
    "m.set_array(colors)\n",
    "# plt.colorbar(m, shrink=0.7)\n",
    "ax.set_xlim3d(minx,maxx), ax.set_ylim3d(miny,maxy), ax.set_zlim3d(vmin,vmax)#(np.min(fs)-1,np.max(fs)+1)\n",
    "# plt.savefig(name+'_min_Ricci.pdf')\n",
    "plt.show()\n",
    "\n",
    "# plot\n",
    "newfig()\n",
    "cmap = cm.jet\n",
    "alpha = 1\n",
    "ax = plt.gca()\n",
    "fs = np.array([np.max(np.real(np.linalg.eigvals(np.dot(M.gsharpf(x.astype(theano.config.floatX)),M.Ricci_curvf(x.astype(theano.config.floatX)))))) for x in xy])\n",
    "# norm = mpl.colors.Normalize(vmin=np.min(fs),vmax=np.max(fs))\n",
    "colors = cmap(norm(fs)).reshape(X.shape+(4,))\n",
    "surf = ax.plot_surface(X, Y, fs.reshape(X.shape), rstride=1, cstride=1, cmap=cmap, facecolors = colors,  linewidth=0., antialiased=True, alpha=alpha, edgecolor=(0,0,0,0), shade=False)\n",
    "m = cm.ScalarMappable(cmap=surf.cmap,norm=norm)\n",
    "m.set_array(colors)\n",
    "# plt.colorbar(m, shrink=0.7)\n",
    "ax.set_xlim3d(minx,maxx), ax.set_ylim3d(miny,maxy), ax.set_zlim3d(vmin,vmax)#(np.min(fs)-1,np.max(fs)+1)\n",
    "# plt.savefig(name+'_max_Ricci.pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.colorbar(m)\n",
    "# plt.savefig(name+'_curvature_colorbar.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Transport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Parallel transport\n",
    "from src.Riemannian import parallel_transport\n",
    "parallel_transport.initialize(M)\n",
    "\n",
    "v = np.array([-1./2,-1./2]).astype(theano.config.floatX)\n",
    "v = v/M.normf(x,v)\n",
    "t = np.linspace(0,1,n_steps.get_value())\n",
    "gamma = 2*np.vstack([-t**2,-np.sin(t)]).T.astype(theano.config.floatX)\n",
    "dgamma = 2*np.vstack([-2*t,-np.cos(t)]).T.astype(theano.config.floatX)\n",
    "\n",
    "# compute  parallel transport\n",
    "vt = M.parallel_transportf(v,gamma,dgamma)\n",
    "\n",
    "# plot result\n",
    "M.newfig()\n",
    "M.plot()\n",
    "M.plotx(gamma,v=vt,N_vec=np.arange(0,n_steps.eval(),2))\n",
    "if name is 'mnist':\n",
    "    plt.xlim([-2.2,.1])\n",
    "# plt.savefig(name+'_parallel_transport.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brownian Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # coordinate form\n",
    "from src.stochastics import Brownian_coords\n",
    "Brownian_coords.initialize(M)\n",
    "\n",
    "tsteps = n_steps.eval(); n_steps.set_value(1000)\n",
    "N = 5\n",
    "xss = np.zeros((N,n_steps.eval(),M.dim.eval()))\n",
    "for i in range(N):\n",
    "    (ts,xs) = M.Brownian_coordsf(x,dWsf(M.dim.eval()))\n",
    "    xss[i] = xs\n",
    "n_steps.set_value(tsteps)\n",
    "\n",
    "# plot\n",
    "M.newfig()\n",
    "M.plot()\n",
    "colormap = plt.get_cmap('winter')\n",
    "colors=[colormap(k) for k in np.linspace(0, 1, N)]\n",
    "for i in range(N):\n",
    "    M.plotx(xss[i],color=colors[i])\n",
    "M.plotx(x,color='r',s=150)\n",
    "# plt.savefig(name+'_Brownian.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# using embedded space approximation and inverse function from manifold to chart\n",
    "from src.stochastics import Brownian_embedded\n",
    "Brownian_embedded.initialize(M)\n",
    "\n",
    "(ts,xs) = M.Brownian_embeddedf(x,dWsf(M.dim.eval()))\n",
    "\n",
    "# plot\n",
    "M.newfig()\n",
    "M.plot()\n",
    "M.plotx(xs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time results of Brownian motion integrations\n",
    "\n",
    "%time xs = M.Brownian_coordsf(x,dWsf(M.dim.eval()))\n",
    "# %time xs = M.Brownian_embeddedf(x,dWsf(M.dim.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Delyon/Hu guided process\n",
    "from src.stochastics.guided_process import *\n",
    "\n",
    "# guide function\n",
    "Cholesky = T.slinalg.Cholesky()\n",
    "# phi = lambda q,v: T.tensordot(T.nlinalg.MatrixInverse()(Cholesky(M.gsharp(q))),-(q-v).flatten(),(1,0))\n",
    "phi = lambda q,v: T.tensordot(Cholesky(M.g(q)).T,-(q-v).flatten(),(1,0))\n",
    "x0 = M.element()\n",
    "(Brownian_coords_guided,Brownian_coords_guidedf) = get_guided_likelihood(M,M.sde_Brownian_coords,phi,lambda x: Cholesky(M.gsharp(x)),x0)\n",
    "\n",
    "if name is 'S2':\n",
    "    x0 = x\n",
    "    x1 = M.Exptf(x,np.array([.8,-.5]).astype(theano.config.floatX))[-1]\n",
    "if name is 'mnist':\n",
    "    x0 = np.random.permutation(x_test_encoded[y_test == 9,:])[0] # pick a number 9\n",
    "    x1 = np.random.permutation(x_test_encoded[y_test == 4,:])[0] # pick a number 4\n",
    "\n",
    "tsteps = n_steps.eval(); n_steps.set_value(1000)\n",
    "N = 5\n",
    "xss = np.zeros((N,n_steps.eval(),M.dim.eval()))\n",
    "for i in range(N):\n",
    "    if name is 'S2':\n",
    "        noise = .2*dWsf(M.dim.eval())\n",
    "    if name is 'mnist':\n",
    "        noise = dWsf(M.dim.eval())\n",
    "    (ts,xs,log_likelihood,log_varphi) = Brownian_coords_guidedf(x0,x1,noise)[:4]\n",
    "    xss[i] = xs\n",
    "    print(\"log likelihood: \", log_likelihood[-1], \", log varphi: \", log_varphi[-1])\n",
    "\n",
    "# plot\n",
    "M.newfig()\n",
    "M.plot()\n",
    "colormap = plt.get_cmap('winter')\n",
    "colors=[colormap(k) for k in np.linspace(0, 1, N)]\n",
    "for i in range(N):\n",
    "    M.plotx(xss[i],color=colors[i])\n",
    "M.plotx(x0,color='r',s=150)\n",
    "M.plotx(x1,color='k',s=150)\n",
    "# plt.savefig(name+'_bridges.pdf')\n",
    "plt.show()\n",
    "\n",
    "if name is 'mnist':\n",
    "    def plot_images(xs,n_t):\n",
    "        indices = np.ceil(np.linspace(0,xs.shape[0]-1,n_t)).astype('int')\n",
    "\n",
    "        # plot\n",
    "        image = np.zeros((28,28*len(indices)))\n",
    "        for j,i in enumerate(indices):\n",
    "            image[:,j*28:(j+1)*28] = M.Ff(xs[i,:]).reshape((28,28))\n",
    "        plt.figure(figsize=(15, 3))\n",
    "        plt.imshow(image, cmap='Greys_r')\n",
    "        plt.axis('off')\n",
    "\n",
    "    for i in range(N):\n",
    "        (ts,xs,log_likelihood,log_varphi) = Brownian_coords_guidedf(x0,x1,2*dWsf(M.dim.eval()))[:4]\n",
    "        plot_images(xs,11)    \n",
    "#         plt.savefig(name+'_image_bridges'+str(i)+'.pdf')\n",
    "        plt.show()\n",
    "\n",
    "n_steps.set_value(tsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples\n",
    "if name is 'S2':\n",
    "    samples = x_test_encoded[np.random.permutation(x_test_encoded.shape[0])]\n",
    "if name is 'mnist':\n",
    "    odds = np.random.permutation(np.where(y_test % 2 == 0)[0])\n",
    "    samples = x_test_encoded[odds] # select odd numbers\n",
    "    samples_y = y_test[odds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithm map\n",
    "from src.Riemannian import Log\n",
    "Log.initialize(M,M.Exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from src.statistics.Frechet_mean import *\n",
    "\n",
    "%time res = Frechet_mean(M,lambda *args: M.Logf(*args),samples[:256],options={'disp': True} )\n",
    "Fm = res[0]\n",
    "print(\"loss = \", res[1])\n",
    "print(\"mean = \", Fm)\n",
    "iterations = res[2]\n",
    "\n",
    "# plot\n",
    "M.newfig()\n",
    "if name is 'S2':\n",
    "    M.plot()\n",
    "    for i in range(256):\n",
    "        M.plotx(samples[i])\n",
    "if name is 'mnist':\n",
    "    plt.scatter(samples[:256, 0], samples[:256, 1], marker='.', s=70, cmap=plt.get_cmap('hsv'))\n",
    "M.plotx(Fm,s=150)\n",
    "M.plotx(iterations,color='green',linewidth=2.5)\n",
    "# plt.savefig(name+'_FM_iterations.pdf')\n",
    "plt.show()\n",
    "\n",
    "if name is 'mnist':\n",
    "    # plot image\n",
    "    plt.imshow(M.Ff(Fm).reshape((28,28)), cmap='Greys_r')\n",
    "#     plt.savefig(name+'_FM.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from src.statistics.tangent_PCA import *\n",
    "\n",
    "pca = tangent_PCA(M,lambda *args: M.Logf(*args),Fm,samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a 2D plot of the digit classes in the PCA coordinates\n",
    "plt.figure(figsize=(18, 12))\n",
    "if name is 'S2':\n",
    "    plt.scatter(pca.transformed_Logs[:, 0], pca.transformed_Logs[:, 1])    \n",
    "if name is 'mnist':\n",
    "    plt.scatter(pca.transformed_Logs[:, 0], pca.transformed_Logs[:, 1], c=samples_y[:pca.transformed_Logs.shape[0]],cmap=plt.get_cmap('hsv'))    \n",
    "#     plt.xlim([-.6,.6]);plt.ylim([-.45,.45]);\n",
    "#     plt.colorbar()\n",
    "# plt.savefig(name+'_pga.pdf')\n",
    "plt.show()\n",
    "\n",
    "if name is 'mnist':\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    plt.figure(figsize=(18, 12))\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], c=samples_y, cmap=plt.get_cmap('hsv'))\n",
    "#     plt.colorbar()\n",
    "#     plt.savefig(name+'_latent.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    # visualize geodesics along major principal component\n",
    "    v0 = pca.components_[:,0]\n",
    "    print(v0)\n",
    "    print(np.sqrt(pca.explained_variance_))\n",
    "\n",
    "    sigma = 10*np.sqrt(pca.explained_variance_[0])\n",
    "    xs = M.Exptf(Fm,-sigma*v0)\n",
    "    xs = np.concatenate((xs[::-1],M.Exptf(Fm,sigma*v0)))\n",
    "\n",
    "    plot_images(xs,11)\n",
    "#     plt.savefig(name+'_first_pcomp.pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    # visualize geodesics along minor principal component\n",
    "    v1 = pca.components_[:,1]\n",
    "    print(v1)\n",
    "\n",
    "    sigma = 10*np.sqrt(pca.explained_variance_[1])\n",
    "    xs = M.Exptf(Fm,-sigma*v1)\n",
    "    xs = np.concatenate((xs[::-1],M.Exptf(Fm,sigma*v1)))\n",
    "\n",
    "    plot_images(xs,11)\n",
    "#     plt.savefig(name+'_second_pcomp.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML mean estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {}\n",
    "if name is 'S2':\n",
    "    options['samples_per_obs'] = 1\n",
    "    options['epochs'] = 200\n",
    "    options['learning_rate'] = .5e-1#1.5e-3\n",
    "    options['varphi_update_rate'] = 1.\n",
    "    options['initial'] = [np.zeros(M.dim.eval()),]\n",
    "    options['verbose'] = True\n",
    "if name is 'mnist':\n",
    "    options['samples_per_obs'] = 1\n",
    "    options['epochs'] = 200\n",
    "    options['learning_rate'] = 1.e-1#1.5e-3\n",
    "    options['varphi_update_rate'] = 1.\n",
    "    options['initial'] = [np.zeros(M.dim.eval()),]\n",
    "    options['verbose'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition density\n",
    "# transition density etc.\n",
    "q0 = M.element()\n",
    "v = M.element()\n",
    "thetas = (q0,)\n",
    "log_p_Tf = theano.function([q0,v],log_p_T(q0,v,dWs(M.dim),Brownian_coords_guided,phi,options,sde=M.sde_Brownian_coords))\n",
    "dlog_p_Tf = theano.function([q0,v],dlog_p_T(thetas,q0,v,dWs(M.dim),Brownian_coords_guided,phi,options,sde=M.sde_Brownian_coords))\n",
    "p_Tf = theano.function([q0,v],T.exp(log_p_T(q0,v,dWs(M.dim),Brownian_coords_guided,phi,options,sde=M.sde_Brownian_coords)))\n",
    "\n",
    "v = M.Exp_Hamiltonianf(q,p)\n",
    "%time print(log_p_Tf(q,v))\n",
    "%time print(p_Tf(q,v))\n",
    "%time print(dlog_p_Tf(q,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.statistics.mle import *\n",
    "\n",
    "def llog_p_T(thetas,pars):\n",
    "    (v,seed) = pars\n",
    "    if seed:\n",
    "        srng.seed(seed)\n",
    "    q = thetas[0]\n",
    "    return dlog_p_Tf(q,v)\n",
    "\n",
    "def update_thetas(thetas, dthetas):\n",
    "    q = thetas[0]\n",
    "    \n",
    "    q += options['learning_rate']*np.dot(M.gsharpf(q),dthetas[0]) # use Riemannian g-gradient\n",
    "    \n",
    "    return (q,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run MLE\n",
    "(thetas, log_likelihood, log_likelihoods, thetass) = iterative_mle(samples[:256],llog_p_T,update_thetas,options)\n",
    "\n",
    "# plot\n",
    "plt.plot(range(options['epochs']),log_likelihoods)\n",
    "# plt.savefig(name+'_ML_likelihoods.pdf')\n",
    "plt.show()\n",
    "plt.plot(range(options['epochs']),thetass[0].reshape((thetass[0].shape[0],-1)))\n",
    "# plt.savefig(name+'_ML_thetas.pdf')\n",
    "plt.show()\n",
    "\n",
    "M.newfig()\n",
    "if name is 'S2':\n",
    "    M.plot()\n",
    "plt.scatter(samples[:256, 0], samples[:256, 1], marker='.', s=70, cmap=plt.get_cmap('hsv'))\n",
    "M.plotx(thetas[0],s=150)\n",
    "M.plotx(thetass[0],color='green',linewidth=2.5)\n",
    "M.plotx(Fm,s=150,color='red')\n",
    "M.plotx(np.vstack((np.zeros((1,2)),iterations)),color='red',linewidth=2.5)\n",
    "# plt.savefig(name+'_MLmean_iterations.pdf')\n",
    "plt.show()\n",
    "\n",
    "if name is 'mnist':\n",
    "    # plot image\n",
    "    plt.imshow(M.Ff(thetas[0]).reshape((28,28)), cmap='Greys_r')\n",
    "#     plt.savefig(name+'_MLmean.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run MLE, digit 8\n",
    "if name is 'mnist':\n",
    "    np.random.seed(42)\n",
    "    samples8 = np.random.permutation(x_test_encoded[y_test == 8,:]) # pick a number 8\n",
    "    \n",
    "    (thetas, log_likelihood, log_likelihoods, thetass) = iterative_mle(samples8[:64],llog_p_T,update_thetas,options)\n",
    "\n",
    "    # plot\n",
    "    plt.plot(range(options['epochs']),log_likelihoods)\n",
    "#     plt.savefig(name+'_ML_likelihoods_8.pdf')\n",
    "    plt.show()\n",
    "    plt.plot(range(options['epochs']),thetass[0].reshape((thetass[0].shape[0],-1)))\n",
    "    # plt.hlines(thetas_true[0].flatten(),plt.xlim()[0],plt.xlim()[1],color='r')\n",
    "#     plt.savefig(name+'_ML_thetas_8.pdf')\n",
    "    plt.show()\n",
    "\n",
    "    M.newfig()\n",
    "    if name is 'S2':\n",
    "        M.plot()\n",
    "    plt.scatter(samples8[:, 0], samples8[:, 1], marker='.', s=70, cmap=plt.get_cmap('hsv'))\n",
    "    M.plotx(thetas[0],s=150)\n",
    "    M.plotx(thetass[0],color='green',linewidth=2.5)\n",
    "#     plt.savefig(name+'_MLmean_iterations_8.pdf')\n",
    "    plt.show()\n",
    "\n",
    "    if name is 'mnist':\n",
    "        # plot image\n",
    "        plt.imshow(M.Ff(thetas[0]).reshape((28,28)), cmap='Greys_r')\n",
    "#         plt.savefig(name+'_MLmean_8.pdf')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
