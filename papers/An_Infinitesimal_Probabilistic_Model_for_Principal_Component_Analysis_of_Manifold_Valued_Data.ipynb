{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This file is part of Theano Geometry\n",
    "#\n",
    "# Copyright (C) 2017, Stefan Sommer (sommer@di.ku.dk)\n",
    "# https://bitbucket.org/stefansommer/theanogemetry\n",
    "#\n",
    "# Theano Geometry is free software: you can redistribute it and/or modify\n",
    "# it under the terms of the GNU General Public License as published by\n",
    "# the Free Software Foundation, either version 3 of the License, or\n",
    "# (at your option) any later version.\n",
    "#\n",
    "# Theano Geometry is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\\\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "# GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License\n",
    "# along with Theano Geometry. If not, see <http://www.gnu.org/licenses/>.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold PPCA on $\\mathbb{S}^2$ and ellipsoids\n",
    "arXiv:1801.10341 [math.ST] http://arxiv.org/abs/1801.10341\n",
    "\n",
    "Stefan Sommer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "surface = 'S2'\n",
    "# surface = 'ellipsoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if surface is 'S2':\n",
    "    from src.manifolds.S2 import *\n",
    "    M = S2(use_spherical_coords=True,chart_center='x')\n",
    "else:\n",
    "    from src.manifolds.ellipsoid import *\n",
    "    M = Ellipsoid(params=[2.,1.,1.],chart_center='x')\n",
    "print(M)\n",
    "\n",
    "from src.plotting import *\n",
    "# %matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riemannian Geodesics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Riemannian structure\n",
    "from src.Riemannian import metric\n",
    "metric.initialize(M,truncate_high_order_derivatives=True)\n",
    "\n",
    "# element, tangent vector and covector\n",
    "x = np.array([np.pi/2,np.pi/2])\n",
    "v = np.dot(np.linalg.cholesky(M.gsharpf(x)),np.array([np.sqrt(2)/2,-np.sqrt(2)/4])*np.pi*.85)\n",
    "p = M.flatf(x,v)\n",
    "\n",
    "print(\"x = \", x)\n",
    "print(\"v = \", v)\n",
    "print(\"p = \", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 2nd order geodesic equation\n",
    "from src.Riemannian import geodesic\n",
    "geodesic.initialize(M)\n",
    "\n",
    "xs = M.Exptf(x,v)\n",
    "\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plotx(xs,v,linewidth = 1.5, s=50)\n",
    "plt.savefig('Riemannian_geodesic_' + surface + '.pdf')\n",
    "plt.show()\n",
    "\n",
    "vv = v\n",
    "v = xs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brownian Motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# srng.seed(42)\n",
    "n_steps.set_value(200) # set to 400 for more accurate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinate form\n",
    "from src.stochastics import Brownian_coords\n",
    "Brownian_coords.initialize(M)\n",
    "\n",
    "(ts,xs) = M.Brownian_coordsf(x,dWsf(M.dim.eval()))\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plotx(xs)\n",
    "plt.savefig('Riemannian_Brownian_motion_' + surface + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anisotropic Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from src.framebundle import FM\n",
    "from src.stochastics import stochastic_development\n",
    "FM.initialize(M)\n",
    "stochastic_development.initialize(M)\n",
    "\n",
    "# covariance\n",
    "nu = np.dot(np.linalg.cholesky(M.gsharpf(x)),np.diag((1.,.1)),)\n",
    "print(nu)\n",
    "u = np.concatenate((x,nu.flatten()))\n",
    "\n",
    "# plot with frame\n",
    "(ts,us) = M.stochastic_developmentf(u,dWsf(M.dim.eval()))\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plotFMx(us)\n",
    "plt.savefig('Anisotropic_FM_' + surface + '.pdf')\n",
    "plt.show()\n",
    "\n",
    "# plot only trajectory\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plotx(us[:,0:M.dim.eval()])\n",
    "plt.savefig('Anisotropic_M_' + surface + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples and Density Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot sample data with trajectories\n",
    "K = 8\n",
    "obss = np.zeros((K,n_steps.eval(),M.dim.eval()))\n",
    "# srng.seed(422)\n",
    "i = 0\n",
    "while i < K:\n",
    "    try:\n",
    "        (ts,us) = M.stochastic_developmentf(u,dWsf(M.dim.eval()))\n",
    "        obss[i] = us[:,0:M.dim.eval()]\n",
    "        i += 1\n",
    "    except np.linalg.linalg.LinAlgError:\n",
    "        pass\n",
    "\n",
    "# plot samples\n",
    "colormap = plt.get_cmap('winter')\n",
    "colors=[colormap(k) for k in np.linspace(0, 1, K)]\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plotFMx(u)\n",
    "for i in range(K):\n",
    "    M.plotx(obss[i],linewidth=.5,color=colors[i])\n",
    "plt.savefig('Samples_with_trajectory_' + surface + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covariance\n",
    "nu = np.dot(np.linalg.cholesky(M.gsharpf(x)),np.diag((np.pi,.1)),)\n",
    "print(nu)\n",
    "u = np.concatenate((x,nu.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sample data\n",
    "K = 256 # or 2048\n",
    "obss = np.zeros((K,M.dim.eval()))\n",
    "# srng.seed(422)\n",
    "i = 0\n",
    "while i < K:\n",
    "    try:\n",
    "        (ts,us) = M.stochastic_developmentf(u,dWsf(M.dim.eval()))\n",
    "        obss[i] = M.get_coordsf(M.Ff(us[-1][0:M.dim.eval()]),x)\n",
    "        i += 1\n",
    "    except np.linalg.linalg.LinAlgError:\n",
    "        pass\n",
    "\n",
    "# plot samples\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plotFMx(u)\n",
    "for i in range(K):\n",
    "    M.plotx(obss[i])\n",
    "plt.savefig('Samples' + surface + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Logarithm map\n",
    "from src.Riemannian import Log\n",
    "Log.initialize(M)\n",
    "\n",
    "# Tangent PCA\n",
    "from src.statistics.tangent_PCA import *\n",
    "\n",
    "from src.utils import *\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = tangent_PCA(M, lambda *args: M.Logf(*args),x,obss)\n",
    "print(pca.get_covariance())\n",
    "\n",
    "plt.scatter(pca.transformed_Logs[:, 0], pca.transformed_Logs[:, 1])\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot estimated density, \n",
    "newfig()\n",
    "plot_density_estimate(M,obss,limits=[-np.pi,np.pi,0,np.pi],pts=100,alpha=.8,bandwidth=.15) # general ellipsoidal coordinates (note: very long computation time)\n",
    "plt.savefig('Density_' + surface + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components and Conditioned Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delyon/Hu guided process\n",
    "from src.stochastics.guided_process import *\n",
    "\n",
    "# Riemannian Brownian motion\n",
    "# guide function\n",
    "phi = lambda g,v: -(g-v)\n",
    "x0 = M.element()\n",
    "(Brownian_coords_guided,Brownian_coords_guidedf) = get_guided_likelihood(\n",
    "    M, M.sde_Brownian_coords, phi,\n",
    "    lambda x: theano.tensor.slinalg.Cholesky()(M.gsharp(x)),\n",
    "    x0)\n",
    "\n",
    "(ts,xs,log_likelihood,log_varphi) = Brownian_coords_guidedf(x,v,dWsf(M.dim.eval()))[:4]\n",
    "print(\"log likelihood: \", log_likelihood[-1], \", log varphi: \", log_varphi[-1])\n",
    "\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plotx(xs)\n",
    "M.plotx(v,color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HMC sampler\n",
    "from src.statistics.HMC import *\n",
    "\n",
    "def U(w,x,v):\n",
    "    Cxv = T.sum(phi(x, v) ** 2)\n",
    "    (_,_,log_likelihood,log_varphi) = Brownian_coords_guided(x,v,w)[:4]\n",
    "    return -log_likelihood[-1]*log_varphi[-1]\n",
    "\n",
    "# for numeric version\n",
    "thetas = ()\n",
    "ve = M.element()\n",
    "(dU,dUf) = get_df(U,dWt,thetas,(x0,ve))\n",
    "Uf = lambda *p: dUf(*p)[0]\n",
    "dwUf = lambda *p: dUf(*p)[1]\n",
    "dthetaUf = lambda *p: dUf(*p)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run HMC\n",
    "w = 0*dWsf(M.dim.eval())\n",
    "K = 10\n",
    "L = 4\n",
    "epsilon = .2/L\n",
    "ws = np.zeros((K,) + w.shape)\n",
    "xss = np.zeros((K,) + xs.shape)\n",
    "vals = np.zeros(K)\n",
    "for k in range(K):\n",
    "    (ts,xs,log_likelihood,log_varphi,w_guided) = Brownian_coords_guidedf(x,v,w)[:5]\n",
    "    ws[k] = w_guided\n",
    "    xss[k] = xs\n",
    "    vals[k] = log_likelihood[-1]*log_varphi[-1]\n",
    "    \n",
    "    w = HMC_step_numeric(Uf, dwUf, lambda: dWsf(M.dim.eval()), epsilon, L, w, (x, v))\n",
    "#     (accept,new_w) = HMC_stepf(w,epsilon,L)\n",
    "#     if accept[0]:\n",
    "#         w = new_w\n",
    "\n",
    "# colors\n",
    "cmap = cm.jet\n",
    "cs = cmap(255*(vals-np.min(vals))/(np.max(vals)-np.min(vals)))\n",
    "\n",
    "# plot result\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plotx(v,color='k')\n",
    "for k in range(K):\n",
    "    M.plotx(xss[k],color=cs[k])\n",
    "plt.show()\n",
    "\n",
    "# noise\n",
    "plt.figure()\n",
    "for k in range(K):\n",
    "    plt.plot(ws[k][:,0],ws[k][:,1],color=cs[k])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Delyon/Hu guided version of development Brownian motion\n",
    "\n",
    "# guide function\n",
    "phi = lambda u,v: T.tensordot(T.nlinalg.MatrixInverse()(u[M.dim:].reshape((M.dim,-1))),-(u[0:M.dim]-v).flatten(),(1,0))\n",
    "u0 = M.FM_element()\n",
    "(Brownian_development_guided,Brownian_development_guidedf) = get_guided_likelihood(\n",
    "    M, M.sde_development, \n",
    "#     phi,\n",
    "    lambda u,v: theano.gradient.disconnected_grad(phi(u,v)),\n",
    "    lambda u: u[M.dim:].reshape((M.dim,-1)), \n",
    "    u0,\n",
    "    A=lambda u, v, w: T.tensordot(v[0:M.dim], T.tensordot(theano.gradient.disconnected_grad(T.nlinalg.MatrixInverse()(T.tensordot(u[M.dim:].reshape((M.dim,-1)), u[M.dim:].reshape((M.dim,-1)), (1, 1)))), w[0:M.dim], 1), 1)\n",
    "#     A=lambda u, v, w: T.tensordot(v[0:M.dim], T.tensordot(T.nlinalg.MatrixInverse()(T.tensordot(u[M.dim:].reshape((M.dim,-1)), u[M.dim:].reshape((M.dim,-1)), (1, 1))), w[0:M.dim], 1), 1)    \n",
    ")\n",
    "\n",
    "# # initial frame bundle element\n",
    "# nu = np.linalg.cholesky(M.gsharpf(x))\n",
    "# u = np.concatenate((x,nu.flatten()))\n",
    "\n",
    "(ts,us,log_likelihood,log_varphi) = Brownian_development_guidedf(u,v,.2*dWsf(M.dim.eval()))[:4]\n",
    "print(\"log likelihood: \", log_likelihood[-1], \", log varphi: \", log_varphi[-1])\n",
    "xs = us[:,:M.dim.eval()]\n",
    "\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plotFMx(us)\n",
    "M.plotx(v,color='k')\n",
    "plt.savefig('Guided_FM_' + surface + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hamiltonian MCMC, development version\n",
    "def U(w,u,v):\n",
    "    Cuv = T.sum(phi(u, v) ** 2)\n",
    "    (_,_,log_likelihood,log_varphi) = Brownian_development_guided(u,v,w)[:4]\n",
    "    return -(-.5 * M.dim.eval() * T.log(2. * np.pi * Tend)\n",
    "             -M.determinant(u[0:M.dim],u[M.dim:].reshape((M.dim,-1)))             \n",
    "             -Cuv/(2.*Tend)\n",
    "             +log_likelihood[-1]*log_varphi[-1])\n",
    "\n",
    "# for numeric version\n",
    "thetas = (u0,)\n",
    "ve = M.element()\n",
    "(dU,dUf) = get_df(U,dWt,thetas,(u0,ve))\n",
    "Uf = lambda *p: dUf(*p)[0]\n",
    "dwUf = lambda *p: dUf(*p)[1]\n",
    "dthetaUf = lambda *p: dUf(*p)[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract principal component from samples\n",
    "def get_principal_component(ws,w_guideds,N_samplers=1):\n",
    "    N_samples = w_guideds.shape[0]//N_samplers\n",
    "    start = (N_samples//N_samplers)//2\n",
    "    ws = ws.reshape((N_samplers,-1,ws.shape[1],ws.shape[2]))[:,start:,:,:].reshape((-1,ws.shape[1],ws.shape[2]))\n",
    "    wsmean = np.mean(ws,axis=0)\n",
    "    w_guideds = w_guideds.reshape((N_samplers,-1,w_guideds.shape[1],w_guideds.shape[2]))[:,start:,:,:].reshape((-1,w_guideds.shape[1],w_guideds.shape[2]))\n",
    "    w_guidedsmean = np.mean(w_guideds,axis=0)\n",
    "    w_guidedmean = w_guidedsmean[-1]\n",
    "    (ts,usmean) = Brownian_development_guidedf(u,v,wsmean)[:2]\n",
    "    return (w_guidedmean,w_guidedsmean,usmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# guess for initial path\n",
    "def MPP(ws,u,v):\n",
    "    (ts,us) = M.stochastic_development(u,ws)\n",
    "    return T.square(ws).sum()/dt+2*1e1*T.square(us[-1][0:M.dim]-v).sum()\n",
    "\n",
    "# for numeric version\n",
    "ws = T.matrix()\n",
    "ue = M.FM_element()\n",
    "ve = M.element()\n",
    "(dMPP,dMPPf) = get_df(MPP,ws,(),(ue,ve))\n",
    "MPPf = lambda *p: dMPPf(*p)[0]\n",
    "dwMPPf = lambda *p: dMPPf(*p)[1]\n",
    "\n",
    "def MPP(u,v):\n",
    "    scale = 1/n_steps.eval()\n",
    "    def fopts(w):\n",
    "        y = dMPPf(w.reshape(n_steps.eval(),-1)*scale,u,v)\n",
    "        print(y[0])\n",
    "        return (y[0],y[1].flatten())\n",
    "    \n",
    "    w = np.zeros(dWsf(M.dim.eval()).shape)\n",
    "    for i in range(150):\n",
    "        grad = dwMPPf(w,u,v)\n",
    "#         print(MPPf(w,u,v),np.linalg.norm(grad))\n",
    "        w -= .1*1e-4*grad\n",
    "\n",
    "#     print(\"after grad descent: %f,%f\" % (MPPf(w,u,v),np.linalg.norm(grad)))\n",
    "#     res = minimize(fopts, np.zeros(dWsf(M.dim.eval()).shape), \n",
    "#                    method='BFGS', jac=True, options={'disp': True, \n",
    "#                                                     'maxiter': 50})\n",
    "    return w#res.x.reshape((n_steps.eval(),-1))/scale\n",
    "\n",
    "w = MPP(u,v)\n",
    "(ts,us) = M.stochastic_developmentf(u,w)\n",
    "# p = M.Log_FM(u,v)\n",
    "# us = M.Exp_Hamiltonian_FMtf(u,p).T\n",
    "# print(us)\n",
    "\n",
    "# plot result\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plotx(v,color='k')\n",
    "# M.plotx(us[:,:M.dim.eval()])\n",
    "M.plotFMx(us,N_vec=n_steps.eval()//4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # covariance\n",
    "# nu = np.dot(np.linalg.cholesky(M.gsharpf(x)),np.diag((.4,1.)))\n",
    "# print(nu)\n",
    "# u = np.concatenate((x,nu.flatten()))\n",
    "\n",
    "# initial point, good guess\n",
    "# w = 0*dWsf(M.dim.eval())\n",
    "# w = .5*ws\n",
    "# print(\"norm at w=0: %f\" % np.linalg.norm(dwUf(w,u,v)))\n",
    "# for i in range(20):\n",
    "#     grad = dwUf(w,u,v)\n",
    "# #     print(Uf(w,u,v),np.linalg.norm(grad))\n",
    "#     w -= .005*grad\n",
    "# print(\"norm after grad descent: %f\" % np.linalg.norm(dwUf(w,u,v)))\n",
    "# (ts,us) = Brownian_development_guidedf(u,v,w)[:2]\n",
    "\n",
    "# w0 = .5*MPP(u,v)\n",
    "# # plot result\n",
    "# newfig()\n",
    "# M.plot()\n",
    "# M.plotx(v,color='k')\n",
    "# # M.plotx(us[:,:M.dim.eval()])\n",
    "# M.plotFMx(us,N_vec=n_steps.eval()//4)\n",
    "# plt.show()\n",
    "\n",
    "# parallel HMC samplers\n",
    "def lHMC(u,pars):\n",
    "    (v,seed,) = pars\n",
    "    if seed:\n",
    "        srng.seed(seed)\n",
    "        \n",
    "    N_samples = 128\n",
    "    L = 8\n",
    "    epsilon = .05/L\n",
    "    maxRetries = 0\n",
    "\n",
    "    w = .5*MPP(u,v)\n",
    "    ws = np.zeros((N_samples,) + w.shape)\n",
    "    w_guideds = np.zeros((N_samples,) + w.shape)\n",
    "    xss = np.zeros((N_samples,n_steps.eval(),M.dim.eval()))\n",
    "    vals = np.zeros(N_samples)\n",
    "    prev_w = w\n",
    "    for k in range(N_samples):\n",
    "        done = False\n",
    "        i = 0\n",
    "        while not done:\n",
    "            try:\n",
    "                w = HMC_step_numeric(Uf, dwUf, lambda: dWsf(M.dim.eval()), epsilon, L, w, (u, v))\n",
    "\n",
    "                (ts,us,log_likelihood,log_varphi,w_guided) = Brownian_development_guidedf(u,v,w)[:5]\n",
    "                ws[k] = w\n",
    "                w_guideds[k] = w_guided\n",
    "                xss[k] = us[:,:M.dim.eval()]\n",
    "                vals[k] = log_likelihood[-1]*log_varphi[-1]\n",
    "\n",
    "                prev_w = w\n",
    "                done = True\n",
    "            except np.linalg.linalg.LinAlgError:\n",
    "                i = i+1\n",
    "                print(\"LinAlgError, retrying (%d of %d)\" % (i,maxRetries))\n",
    "                if i >= maxRetries:\n",
    "                    print('backtracking')                    \n",
    "                    w = prev_w\n",
    "                    done = True\n",
    "#                     raise\n",
    "    (w_guidedmean,w_guidedsmean,usmean) = get_principal_component(ws,w_guideds)\n",
    "\n",
    "    return (ws,w_guideds,xss,vals,w_guidedmean,w_guidedsmean,usmean)\n",
    "\n",
    "# run HMC\n",
    "N_samplers = 8\n",
    "print(\"Running HMC with %d sampler(s)\" % N_samplers)\n",
    "try:\n",
    "    mpu.openPool()\n",
    "    sol = mpu.pool.imap(partial(lHMC,u),mpu.inputArgs(itertools.cycle((v,)),np.random.randint(1000,size=N_samplers)))\n",
    "    res = list(sol)\n",
    "    ws = mpu.getRes(res,0).reshape((-1,n_steps.eval(),M.dim.eval()))\n",
    "    w_guideds = mpu.getRes(res,1).reshape((-1,n_steps.eval(),M.dim.eval()))\n",
    "    xss = mpu.getRes(res,2).reshape((-1,n_steps.eval(),M.dim.eval()))\n",
    "    vals = mpu.getRes(res,3).flatten()\n",
    "    w_guidedmean = np.mean(mpu.getRes(res,4),axis=0)\n",
    "    w_guidedsmean = np.mean(mpu.getRes(res,5),axis=0)\n",
    "    usmean = np.mean(mpu.getRes(res,6),axis=0)    \n",
    "except:\n",
    "    mpu.closePool()\n",
    "    raise\n",
    "else:\n",
    "    mpu.closePool()\n",
    "print(\"HMC done, %d samples total\" % vals.size)\n",
    "\n",
    "# colors\n",
    "cmap = cm.jet\n",
    "cs = cmap(255*(vals-np.min(vals))/(np.max(vals)-np.min(vals)))\n",
    "\n",
    "# # plot result\n",
    "# newfig()\n",
    "# M.plot()\n",
    "# M.plotx(v,color='k')\n",
    "# skip = np.max((1,K//K))\n",
    "# for k in range(0,K,skip):\n",
    "#     M.plotx(xss[k],color=cs[k])\n",
    "# plt.show()\n",
    "\n",
    "# # noise\n",
    "# plt.figure()\n",
    "# for k in range(0,K,skip):\n",
    "#     plt.plot(w_guideds[k][:,0],w_guideds[k][:,1],'.-',color=cs[k])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # mean path (using development)\n",
    "# wsmean = np.mean(ws,axis=0)\n",
    "# (ts,usmean) = Brownian_development_guidedf(u,v,wsmean)[:2]\n",
    "# w_guidedsmean = np.mean(w_guideds,axis=0)\n",
    "\n",
    "start = 0\n",
    "skip = np.max((1,(vals.size-start)//40))\n",
    "\n",
    "# plot result\n",
    "newfig()\n",
    "M.plot()\n",
    "for k in range(start,vals.size,skip):\n",
    "    M.plotx(xss[k],color=cs[k])\n",
    "# M.plotFMx(usmean,N_vec=n_steps.eval()//4)\n",
    "M.plotx(usmean[:,0:M.dim.eval()],color='blue',linewidth=2)\n",
    "M.plotFMx(u)    \n",
    "M.plotx(M.Exptf(x,vv),linewidth = 1.5, color='r')\n",
    "M.plotx(v,color='r',s=150)\n",
    "plt.savefig('Conditioned_samples_' + surface + '.pdf')\n",
    "plt.show()\n",
    "\n",
    "vvu = np.linalg.solve(nu,vv)\n",
    "\n",
    "# range\n",
    "(xmin,ymin) = np.min(np.vstack((w_guideds.reshape(-1,M.dim.eval()),vvu)),axis=0)-1.\n",
    "(xmax,ymax) = np.max(np.vstack((w_guideds.reshape(-1,M.dim.eval()),vvu)),axis=0)+1.\n",
    "\n",
    "# noise\n",
    "plt.figure()\n",
    "for k in range(start,vals.size,skip):\n",
    "    plt.plot(w_guideds[k][:,0],w_guideds[k][:,1],color=cs[k])\n",
    "    plt.plot(w_guideds[k][-1,0],w_guideds[k][-1,1],'o',color='k',markersize=10)\n",
    "plt.plot(w_guidedsmean[:,0],w_guidedsmean[:,1],'-',color='b',linewidth=4)\n",
    "plt.plot([0.,vvu[0]],[0.,vvu[1]],'r-',linewidth=2)\n",
    "# plt.axis((-.5,4.2,-2.5,.5))\n",
    "plt.savefig('Conditioned_antidevelopment_' + surface + '.pdf')\n",
    "plt.show()\n",
    "\n",
    "# noise density\n",
    "plt.figure()\n",
    "plot_Euclidean_density_estimate(w_guideds.reshape((-1,2)),limits=(xmin,xmax,ymin,ymax))\n",
    "plt.plot(w_guidedsmean[:,0],w_guidedsmean[:,1],'-',color='b',linewidth=1)\n",
    "plt.plot([0.,vvu[0]],[0.,vvu[1]],'r-',linewidth=2)\n",
    "# plt.axis((-.5,4.2,-2.5,.5))\n",
    "plt.savefig('Conditioned_antidevelopment_density_' + surface + '.pdf')\n",
    "plt.show()\n",
    "\n",
    "newfig()\n",
    "plot_Euclidean_density_estimate(w_guideds.reshape((-1,2)),view='3D',border=1.,alpha=.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclideanization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# data plot\n",
    "newfig()\n",
    "M.plot()\n",
    "plot_density_estimate(M,obss,limits=[-np.pi,np.pi,0,np.pi],pts=100,alpha=.2,bandwidth=.15) # general ellipsoidal coordinates (note: very long computation time)\n",
    "\n",
    "# plot samples\n",
    "M.plotFMx(u)\n",
    "K = obss.shape[0]\n",
    "for i in range(K):\n",
    "    M.plotx(obss[i],color='k')\n",
    "M.plotx(v)\n",
    "\n",
    "# plot geodesic and mean curve\n",
    "M.plotx(usmean[:,0:M.dim.eval()],color='k',linewidth=4)\n",
    "M.plotFMx(u)    \n",
    "M.plotx(M.Exptf(x,vv), color='r', linewidth=4)\n",
    "M.plotx(v,color='b',s=50,linewidth=5)\n",
    "\n",
    "plt.savefig('Data_geodesic_component_' + surface + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute anisotropic PCA view\n",
    "# run HMC\n",
    "N_samplers = 1\n",
    "K = 128 #obss.shape[0]\n",
    "batches = 2\n",
    "print(\"Running HMC, %d observations per batch of %d, %d samplers\" % (K,batches,N_samplers))\n",
    "for J in range(batches):\n",
    "    print(\"batch %d of %d\" % (J,batches))\n",
    "    try:\n",
    "        mpu.openPool()\n",
    "        sol = mpu.pool.imap(partial(lHMC,u),mpu.inputArgs(obss[J*K:(J+1)*K],np.random.randint(1000,size=K)))\n",
    "        res = list(sol)\n",
    "        if J == 0:\n",
    "            w_guidedmean = mpu.getRes(res,4).reshape((-1,M.dim.eval()))\n",
    "        else:\n",
    "            w_guidedmean = np.vstack((w_guidedmean,mpu.getRes(res,4).reshape((-1,M.dim.eval()))))\n",
    "    except:\n",
    "        mpu.closePool()\n",
    "        raise\n",
    "    else:\n",
    "        mpu.closePool()\n",
    "print(\"HMC done, %d observations\" % K)\n",
    "print(w_guidedmean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display in nu coordinates\n",
    "tpca_comps = np.linalg.solve(M.orthFramef(x),pca.transformed_Logs.T).T\n",
    "pca_comps = np.linalg.solve(M.orthFramef(x),np.tensordot(nu,w_guidedmean,(1,1))).T\n",
    "tvvu = np.linalg.solve(M.orthFramef(x),np.tensordot(nu,vvu,(1,0)))\n",
    "tw_guidedsmean = np.linalg.solve(M.orthFramef(x),np.tensordot(nu,w_guidedsmean,(1,1))).T\n",
    "plt.scatter(tpca_comps[:, 0], tpca_comps[:, 1],color='r',marker='x')\n",
    "plt.scatter(pca_comps[:, 0],pca_comps[:, 1],color='k')\n",
    "plt.plot([0.,tvvu[0]],[0.,tvvu[1]],'r-',linewidth=5)\n",
    "plt.scatter(tvvu[0], tvvu[1],color='r',marker='x')\n",
    "plt.plot(tw_guidedsmean[:,0],tw_guidedsmean[:,1],'-',color='k',linewidth=5)\n",
    "plt.scatter(tw_guidedsmean[-1,0],tw_guidedsmean[-1,1],color='k')\n",
    "plt.ylim((-2.2,2.2))\n",
    "plt.xlim((-3.5,3.5))\n",
    "# plt.axis('equal')\n",
    "plt.savefig('Tangent_vs_model_PCA_' + surface + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# covariance\n",
    "nu = np.dot(np.diag((.075,.4)),np.linalg.cholesky(M.gsharpf(x)))\n",
    "print(nu)\n",
    "u = np.concatenate((x,nu.flatten()))\n",
    "\n",
    "thetas_true = (u,)\n",
    "\n",
    "n_steps.set_value(100)\n",
    "\n",
    "# sample data\n",
    "# srng.seed(7163)\n",
    "K = 4*22\n",
    "obss = np.zeros((K,M.dim.eval()))\n",
    "# srng.seed(422)\n",
    "i = 0\n",
    "while i < K:\n",
    "    try:\n",
    "        (ts,us) = M.stochastic_developmentf(u,dWsf(M.dim.eval()))\n",
    "        obss[i] = us[-1][0:M.dim.eval()]\n",
    "        i += 1\n",
    "    except np.linalg.linalg.LinAlgError:\n",
    "        pass\n",
    "\n",
    "# plot samples\n",
    "newfig()\n",
    "M.plot()\n",
    "M.plotFMx(u)\n",
    "for i in range(K):\n",
    "    M.plotx(obss[i])\n",
    "plt.savefig('MLE_samples_' + surface + '.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {}\n",
    "options['samples_per_obs'] = 1\n",
    "options['epochs'] = 15\n",
    "options['learning_rate'] = .97e-2\n",
    "options['verbose'] = True\n",
    "options['initial'] = [np.concatenate((x,.5*np.eye(M.dim.eval()).flatten()))] # [np.concatenate((x,np.diag((.1,.25)).flatten()))] # [np.concatenate((x,.5*np.eye(M.dim.eval()).flatten()))] # [.5*thetas_true[0]]\n",
    "options['chain_sampler'] = True\n",
    "options['chain_size'] = dWsf(M.dim.eval()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# optimization\n",
    "from src.statistics.mle import *\n",
    "\n",
    "# np.random.seed(345)\n",
    "\n",
    "def llog_p_T(thetas,pars):\n",
    "    (v,seed,w) = pars\n",
    "    if seed:\n",
    "        srng.seed(seed)\n",
    "    u = thetas[0]\n",
    "    w = HMC_step_numeric(Uf, dwUf, lambda: dWsf(M.dim.eval()), epsilon, L, w, (u, v))\n",
    "        \n",
    "    maxRetries = 10\n",
    "    done = False\n",
    "    i = 0\n",
    "    while not done:\n",
    "        try:\n",
    "            w = HMC_step_numeric(Uf, dwUf, lambda: dWsf(M.dim.eval()), epsilon, L, w, (u, v))\n",
    "            res = dUf(w,u,v)            \n",
    "            done = True\n",
    "        except np.linalg.linalg.LinAlgError:\n",
    "            i = i+1\n",
    "            print(\"LinAlgError, retrying (%d of %d)\" % (i,maxRetries))\n",
    "            if i >= maxRetries:\n",
    "                print('maxRetries reached')\n",
    "                w = 0\n",
    "    \n",
    "    return (res[0],res[2],w)\n",
    "\n",
    "def update_thetas(thetas, dthetas):\n",
    "    u = thetas[0]\n",
    "  \n",
    "    u -= options['learning_rate']*dthetas[0]\n",
    "#     u[M.dim.eval():] -= options['learning_rate']*dthetas[0][M.dim.eval():] # only diagonal covariance\n",
    "#     u[M.dim.eval():] -= options['learning_rate']*(np.eye(M.dim.eval())*dthetas[0][M.dim.eval():].reshape((M.dim.eval(),-1))).flatten() # only covariance\n",
    "#     u[M.dim.eval()+0] -= 2.5*options['learning_rate']*dthetas[0][M.dim.eval()+0]\n",
    "#     u[M.dim.eval()+3] -= .25*options['learning_rate']*dthetas[0][M.dim.eval()+3]\n",
    "    \n",
    "    return (u,)\n",
    "\n",
    "# run MLE\n",
    "(thetas, log_likelihood, log_likelihoods, thetass) = iterative_mle(obss,llog_p_T,update_thetas,options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plot_thetas_true = np.diag(np.tensordot(np.linalg.cholesky(M.gf(x)),thetas_true[0][M.dim.eval():].reshape((M.dim.eval(),-1)),(1,0)))\n",
    "plot_thetass = np.zeros((thetass[0].shape[0]+1,M.dim.eval()))\n",
    "plot_thetass[0] = np.diag(np.tensordot(np.linalg.cholesky(M.gf(x)),options['initial'][0][M.dim.eval():].reshape((M.dim.eval(),-1)),(1,0)))\n",
    "for i in range(thetass[0].shape[0]):\n",
    "    plot_thetass[i+1] = np.diag(np.tensordot(np.linalg.cholesky(M.gf(x)),thetass[0][i][M.dim.eval():].reshape((M.dim.eval(),-1)),(1,0)))\n",
    "plt.plot(range(log_likelihoods.shape[0]),log_likelihoods)\n",
    "plt.savefig('MLE_likelihood_' + surface + '.pdf')\n",
    "plt.show()\n",
    "plt.plot(range(log_likelihoods.shape[0]+1),plot_thetass.reshape((thetass[0].shape[0]+1,-1)))\n",
    "plt.hlines(plot_thetas_true.flatten(),plt.xlim()[0],plt.xlim()[1],color='r')\n",
    "lims = plt.axis(); plt.axis((lims[0],lims[1],0.,lims[3]))\n",
    "plt.savefig('MLE_variance_' + surface + '.pdf')\n",
    "plt.show()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
